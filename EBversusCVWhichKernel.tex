\documentclass{amsart}
\usepackage{mathtools,upref,siunitx,upquote,fancyvrb,xspace,color}
\usepackage[hyphens]{url}
\usepackage[utf8]{inputenc}
\usepackage{esdiff}

\input{FJHDef.tex}

\newcommand{\rvf}{\mathring{\vf}}
\newcommand{\gp}{\mathcal{G}\!\mathcal{P}}


\usepackage{algpseudocode}
\usepackage{algorithm, algorithmicx}
\algnewcommand\algorithmicparam{\textbf{Parameters:}}
\algnewcommand\PARAM{\item[\algorithmicparam]}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\RETURN{\State \textbf{Return }}



\begin{document}
\title{Sample Document}
\author{Fred J. Hickernell}
\author{Houman Owhadi}
\author{Aleksei Sorokin}
\begin{abstract}This project is where all of the files and commands go that are needed elsewhere
\end{abstract}

\maketitle

\section{Introduction}

\section{Background}
Let $f$ have an absolutely summable Fourier series
\begin{equation*}
    f(\vx) = \sum_{\vk \in \integers^d} \hf(\vk) \exp(2 \pi \sqrt{-1} \vk^T \vx), \qquad \hf(\vk) : = \int_{[0,1]^d} f(\vx)  \exp(-2 \pi \sqrt{-1} \vk^T \vx) \, \dif \vx.
\end{equation*}
Suppose that the sampling sites or nodes constitutes $\{ \vx_1, \ldots, \vx_n\}$ is a shifted integration lattice, 
\begin{equation} \label{eq:shift_lat}
    \vx_i = \vh i /n + \vDelta \pmod \vone, \qquad i = 0, \ldots, n - 1
\end{equation}
where $\vh \in \{1, \ldots, n-1\}^d$.  Then the discrete Fourier transform (DFT) of the function data, $\{ f(\vx_0), \ldots, f(\vx_{n)-1}\}$, is given by 
\begin{align*}
    \tf(\vk) &= \frac 1n \sum_{i=1}^n f(\vx_i) \exp(-2 \pi \sqrt{-1} \vk^T \vx_i) \\
    & = \frac 1n \sum_{i=1}^n f(\vx_i) \exp(-2 \pi \sqrt{-1} [\vk^T\vh i/n + \vk^T\vDelta]) \\
    & = \rf(\vk^T \vh \bmod n) \exp(-2 \pi \sqrt{-1} \vk^T\vDelta), \\
    \text{where } 
    \rf(j) &: = \frac 1n \sum_{i=1}^n f(\vx_i) \exp(-2 \pi \sqrt{-1} ij/n), \quad  j = 0, \ldots, n-1 \\
    & = \frac 1n \sum_{i=1}^n \sum_{\vk \in \integers^d} \hf(\vk) \exp(2 \pi \sqrt{-1} \vk^T \vx_i) \exp(-2 \pi \sqrt{-1} ij/n) \\
    & = \frac 1n \sum_{\vk \in \integers^d} \hf(\vk) \sum_{i=1}^n \exp(2 \pi \sqrt{-1} i (\vk^T \vh - j) /n + \vk^T \vDelta) \\
    & = \sum_{\substack{\vk \in \integers^d \\ \vk^T \vh \bmod n = j}} \hf(\vk) \exp(2 \pi \sqrt{-1} \vk^T \vDelta) \\
\end{align*}
Note that the DFT takes on only $n$ distinct values. 

We want to consider functions that are samples from a Gaussian process with a covariance kernel, $K$, and/or elements in a Hilbert space with reproducing kernel $K$.  The $K$ is assumed to be  periodic:
\begin{equation*}
    K(\vt,\vx) = \sum_{\vk \in \integers^d} \omega_{\vk}^2 \exp(2 \pi \sqrt{-1} \vk^T (\vt - \vx))
\end{equation*}
where the set of weights, $\vomega : = (\omega_{\vk})_{\vk \in \integers^d}$, satisfies the following summability condition:
\begin{equation*}
    \sum_{\vk \in \integers^d} \omega_{\vk}^2 < \infty.
\end{equation*}
We must assume that $\omega_{-\vk} = \omega_{\vk}$  because the kernel $K$ must be symmetric in its arguments. 

For such a kernel, the inner product of the associated reproducing kernel Hilbert space is defined by 
\begin{equation*}
    \ip[\vomega]{f}{g} = \sum_{\vk \in \integers^d} 
    \frac{\overline{\hf(\vk)} \hg(\vk)}{\omega_{\vk}^2}
\end{equation*}
The reproducing property follows because $K(\cdot,\vx)$ is in the Hilbert space, and 
\begin{equation*}
    \ip[\vomega]{K(\cdot,\vx)}{f} 
    = \sum_{\vk \in \integers^d} 
    \frac{ \omega_{\vk}^2 \exp(2 \pi \sqrt{-1} \vk^T \vx)) \hf(\vk)}{\omega_{\vk}^2} = f(\vx).
\end{equation*}

Now consider the entries of the Gram matrix for a periodic kernel where the sampling sites or nodes are a shifted lattice as defined in \eqref{eq:shift_lat}:
\begin{align*}
    K(\vx_i,\vx_j)  &= \sum_{\vk \in \integers^d} \omega_{\vk}^2 \exp(2 \pi \sqrt{-1} \vk^T(\vx_i - \vx_j)) \\
    & = \sum_{\vk \in \integers^d} \omega_{\vk}^2 \exp(2 \pi \sqrt{-1} \vk^T \vh (i-j)/n) \\
    & = \frac 1n \sum_{l = 0}^{n-1} \lambda_l \exp(2 \pi \sqrt{-1} l (i-j)/n), \quad \lambda_l =: n\sum_{\substack{\vk \in \integers^d \\ \vk^T\vh \bmod n = l}} \omega_{\vk}^2, \\
    \MoveEqLeft \underbrace{\bigl ( K(\vx_i,\vx_j)\bigr)_{i,j = 0}^{n-1}}_{\mK} \\
    &
    = \frac 1n \underbrace{\bigl ( \exp(2 \pi \sqrt{-1} il/n) \bigr)_{i,l = 0}^{n-1}}_{\mV} 
    \underbrace{\bigl ( \lambda_l \delta_{l,m} \bigr)_{l,m = 0}^{n-1}}_{\mLambda = \diag(\vlambda)} 
    \underbrace{\bigl ( \exp(- 2 \pi \sqrt{-1} mj/n) \bigr)_{m,j = 0}^{n-1}}_{\mV^H}
\end{align*}
Since $\mV^H \mV = n \mI$, it follows that $\mK \mV  =\mV \mLambda)$, and so $\vlambda$ are the eigenvalues of $\mK$.  It also follows that $\mK^{-1} = n^{-1} \mV \mLambda \mV^H$.  Note that the first (or zeroth) column of $\mK$ is 
\begin{equation*}
    \vK_0 = \bigl ( K(\vx_i,\vx_0)\bigr)_{i = 0}^{n-1}
    = \biggl( \frac 1n \sum_{l = 0}^{n-1} \lambda_l \exp(2 \pi \sqrt{-1} l i/n)\biggr)_{i=1}^n = \frac 1n \mV \vlambda,
\end{equation*}
and so 
\begin{equation*}
    \mV^{H} \vK_0 = \mV^{H} \frac 1n \mV \vlambda = \vlambda 
\end{equation*}

Also, letting $\vf := \bigl( f(\vx_i) \bigr)_{i=1}^n$ and $\rvf := \bigl( \rf(\vx_i) \bigr)_{i=1}^n$, it follows that
\begin{equation}
    \rvf = \frac 1n \mV^H \vf, \qquad \vf = \mV \rvf
\end{equation}

\section{Optimizing the Kernel According to Empirical Bayes}
We assume that the function $f$ is an instance of a zero mean Gaussian process, i.e., $f \in \gp(0,s^2K_{\vtheta})$, where we now show the explicit dependence of the
kernel $K$ on the hyperparameters $\vtheta$ as well as the hyperparameter $s^2$, which is the overall vertical scale.
This means that $\{\omega_{\vk}\}_{\vk \in \integers^d}$, and $\{\lambda_l \}_{l=0}^{n-1}$ all depend on $\vtheta$ as well, but $\mV$ does not.

In the empirical Bayes setting, we assume that $f \in \gp(0,s^2K_{\vtheta})$, where we now show the explicit dependence of the
kernel $K$ on the hyperparameters $\vtheta$ as well as the hyperparameter $s^2$, which is the overall vertical scaling.  
The log likelihood (ignoring constants) given the data $\vf$ is writtem as 
\begin{align}
	\text{loglike}(s^2,\vtheta) 
	& = - \frac 1{2} \vf^T (s^2\mK_{\vtheta})^{-1}\vf - \frac 12 \log\bigl(\det(s^2 \mK_{\vtheta})\bigr)  \\
	\nonumber 
	& = - \frac 1{2 s^2} \vf^T\Bigl(\frac 1n \mV \mLambda_{\vtheta}^{-1} \mV^H \Bigr)\vf 
	- \frac 12 \log\biggl(\prod_{l=0}^{n-1} \lambda_{\vtheta,l}\biggr) - n \log(s)  \\
	\nonumber 
	& = - \frac n{2 s^2} \rvf^T\mLambda_{\vtheta}^{-1} \rvf 
- \frac 12 \sum_{l=0}^{n-1} \log(\lambda_{\vtheta,l}) - \frac{n}2 \log(s^2)  \\
\nonumber 
\end{align}
Maximizing the above quantity with respect to $s^2$ 



\section{Optimizing the Kernel according to Empirical Bayes or Generalized Cross-Validation Assuming Arbitrary $\lambda_i$}
According to \cite{RatHic19a}, the empirical Bayes choice of parameters defining the kernel is
\begin{align*}
    \vtheta_{\text{EB}} 
    &= \argmin_{\vtheta} \left [\log\left( \sum_{i=1}^{n-1} \frac{\lvert\rf_i\rvert^2}{\lambda_{\vtheta,i}} \right)
    + \frac 1n \sum_{i=0}^{n-1} \log(\lambda_{\vtheta,i} ) \right ]
\end{align*}

Suppose for a moment that the $\lambda_{\vtheta}$ can be chosen arbitrarily.   We know this is not true, but it is easy to solve the problem under this assumption.  Dropping the $\vtheta$ dependence, we want to satisfy
\begin{gather*}
    \frac 1{\lambda_0} = 0, \qquad 
    - \frac{\lvert\rf_i\rvert^2}{\lambda_{i}^2} \left( \sum_{j=1}^{n-1} \frac{\lvert\rf_j\rvert^2}{\lambda_{j}} \right)^{-1} + \frac 1{n\lambda_i} = 0, \quad i = 1, \ldots, n-1
    \\
    \iff
    \lambda_0 = \infty, \qquad \frac{\lvert\rf_i\rvert^2}{\lambda_{i}} = \frac 1n \sum_{j=1}^{n-1} \frac{\lvert\rf_j\rvert^2}{\lambda_{j}}, \quad i = 1, \ldots, n-1
    \\
    \iff
    \lambda_0 = \infty, \qquad \lambda_i = c \lvert\rf_i\rvert^2, \quad i = 1, \ldots, n-1.
\end{gather*}

According to \cite{RatHic19a}, the generalized cross-validation choice of parameters defining the kernel is
\begin{align*}
    \vtheta_{\text{GCV}} 
    &= \argmin_{\vtheta} \left [\log\left( \sum_{i=1}^{n-1} \frac{\lvert\rf_i\rvert^2}{\lambda_{\vtheta,i}^2} \right)
    - 2 \log \left(\sum_{i=0}^{n-1} \frac{1}{\lambda_{\vtheta,i}} \right) \right ]
\end{align*}

Again, suppose for a moment that the $\lambda_{\vtheta}$ can be chosen arbitrarily.  Dropping the $\vtheta$ dependence, we want to satisfy
\begin{gather*}
    \begin{cases}
    \displaystyle \frac {2}{\lambda_0^2} \left( \sum_{j=0}^{n-1} \frac{1}{\lambda_{j}} \right)^{-1}  = 0, \\ 
    \displaystyle - \frac{2\lvert\rf_i\rvert^2}{\lambda_{i}^3} \left( \sum_{j=1}^{n-1} \frac{\lvert\rf_j\rvert^2}{\lambda_{j}^2} \right)^{-1} +  \frac{2}{\lambda_i^2} \left( \sum_{j=0}^{n-1} \frac{1}{\lambda_{j}} \right)^{-1} = 0, \quad i = 1, \ldots, n-1
    \end{cases}
    \\
    \iff
    \begin{cases}
    \lambda_0 = \infty,  \\ 
    \displaystyle \frac{\lvert\rf_i\rvert^2}{\lambda_{i}} =  \left( \sum_{j=1}^{n-1} \frac{\lvert\rf_j\rvert^2}{\lambda_{j}^2} \right) \, \left( \sum_{j=1}^{n-1} \frac{1}{\lambda_{j}} \right)^{-1} = 0, \quad i = 1, \ldots, n-1
    \end{cases}
    \\
    \iff
     \lambda_0 = \infty, \qquad \lambda_i = c \lvert\rf_i\rvert^2, \quad i = 1, \ldots, n-1.
\end{gather*}

\bibliographystyle{amsplain}
\bibliography{FJH25,FJHown25}

\end{document}